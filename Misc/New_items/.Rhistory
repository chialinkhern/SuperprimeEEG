filtered_df = matched_abs[((matched_abs$conc < conc_upper) & (matched_abs$conc > conc_lower)),]
sample = sample_n(filtered_df, num_samples, replace=FALSE)
new_abs = rbind(new_abs, sample)
index = index + 1
}
View(new_abs)
View(sample)
View(old_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
index = 1
for (conc in names(conc_freq)) {
conc = as.numeric(conc)
num_samples = conc_freq[index]
conc_lower = conc - 10
conc_upper = conc + 10
filtered_df = matched_abs[((matched_abs$conc < conc_upper) & (matched_abs$conc > conc_lower)),]
sample = sample_n(filtered_df, num_samples, replace=FALSE)
new_abs = rbind(new_abs, sample)
index = index + 1
}
View(new_abs)
View(old_abs)
index = 1
for (row in old_abs){
print(row)
}
index = 1
for (row in old_abs){
(row)
}
index = 1
for (row in old_abs){
row
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
index = 1
for (row in old_abs){
rbind(new_abs, row)
}
View(new_abs)
index = 1
for (row in nrow(old_abs)){
rbind(new_abs, old_abs[row,])
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
index = 1
for (row in nrow(old_abs)){
rbind(new_abs, old_abs[row,])
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
View(new_abs)
index = 1
for (row in nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row,])
}
View(new_abs)
row_index = 1
for (row_index in nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row,])
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row,])
row_index = row_index + 1
}
nrow(old_abs)
range(nrow(old_abs))
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row,])
row_index = row_index + 1
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row_index,])
row_index = row_index + 1
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row in old_abs){
new_abs = rbind(new_abs, row)
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row in old_abs){
new_abs = cbind(new_abs, row)
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row in old_abs){
new_abs = rbind(new_abs, row)
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
new_abs = rbind(new_abs, old_abs[row_index,])
row_index = row_index + 1
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = row$conc
len = row$len
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+10)&(matched_abs$word_len>len-10)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
sample
sample_n(1,1)
filtered_abs
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = row$conc
len = row$len
filtered_abs = matched_abs[((matched_abs$conc<conc+20)&(matched_abs$conc>conc-20)&(matched_abs$word_len<len+20)&(matched_abs$word_len>len-20)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
# for (conc in names(conc_freq)) {
#   conc = as.numeric(conc)
#   num_samples = conc_freq[index]
#   conc_lower = conc - 10
#   conc_upper = conc + 10
#   filtered_df = matched_abs[((matched_abs$conc < conc_upper) & (matched_abs$conc > conc_lower)),]
#   sample = sample_n(filtered_df, num_samples, replace=FALSE)
#   new_abs = rbind(new_abs, sample)
#   index = index + 1
# }
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = row$conc
len = row$len
filtered_abs = matched_abs[((matched_abs$conc<conc+50)&(matched_abs$conc>conc-50)&(matched_abs$word_len<len+5)&(matched_abs$word_len>len-5)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
filtered_abs
sample_n(filtered_abs,1)
filtered_abs = matched_abs[((matched_abs$conc<conc+90)&(matched_abs$conc>conc-90)&(matched_abs$word_len<len+5)&(matched_abs$word_len>len-5)),]
conc = row$conc
len = row$len
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$len)
filtered_abs = matched_abs[((matched_abs$conc<conc+90)&(matched_abs$conc>conc-90)&(matched_abs$word_len<len+5)&(matched_abs$word_len>len-5)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
filtered_abs
row$word_len
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+90)&(matched_abs$conc>conc-90)&(matched_abs$word_len<len+5)&(matched_abs$word_len>len-5)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+20)&(matched_abs$conc>conc-20)&(matched_abs$word_len<len+3)&(matched_abs$word_len>len-3)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
View(new_abs)
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+20)&(matched_abs$conc>conc-20)&(matched_abs$word_len<len+3)&(matched_abs$word_len>len-3)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
View(new_abs)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+20)&(matched_abs$conc>conc-20)&(matched_abs$word_len<len+3)&(matched_abs$word_len>len-3)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
View(new_abs)
hist(new_abs$conc)
hist(new_abs$word_len)
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+20)&(matched_abs$conc>conc-20)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
hist(new_abs$conc)
hist(new_abs$word_len)
concHist = (hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness"))
lenHist = (hist(old_abs$word_len, main="Word length of old abstract targets", xlab="Word Length"))
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
View(new_abs)
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
conc = c(round(min(old_abs$conc, na.rm=TRUE), 2), round(max(old_abs$conc, na.rm=TRUE),2), round(mean(old_abs$conc, na.rm=TRUE),2))
word_len = c(round(min(old_abs$word_len, na.rm=TRUE), 2), round(max(old_abs$word_len, na.rm=TRUE),2), round(mean(old_abs$word_len, na.rm=TRUE),2))
desc_stats = rbind(conc, word_len)
colnames(desc_stats) = c("min", "max", "mean")
desc_stats
concHist = (hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness"))
lenHist = (hist(old_abs$word_len, main="Word length of old abstract targets", xlab="Word Length"))
# sample new items according to distribution of old items
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
View(new_abs)
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
# loading in old items
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
# loading in items matched on concreteness and word length from MRC
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
# old conc distribution
hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness"))
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
# loading in old items
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
# loading in items matched on concreteness and word length from MRC
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
conc = c(round(min(old_abs$conc, na.rm=TRUE), 2), round(max(old_abs$conc, na.rm=TRUE),2), round(mean(old_abs$conc, na.rm=TRUE),2))
word_len = c(round(min(old_abs$word_len, na.rm=TRUE), 2), round(max(old_abs$word_len, na.rm=TRUE),2), round(mean(old_abs$word_len, na.rm=TRUE),2))
desc_stats = rbind(conc, word_len)
colnames(desc_stats) = c("min", "max", "mean")
desc_stats
# old conc distribution
hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness"))
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
# loading in old items
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
# loading in items matched on concreteness and word length from MRC
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
conc = c(round(min(old_abs$conc, na.rm=TRUE), 2), round(max(old_abs$conc, na.rm=TRUE),2), round(mean(old_abs$conc, na.rm=TRUE),2))
word_len = c(round(min(old_abs$word_len, na.rm=TRUE), 2), round(max(old_abs$word_len, na.rm=TRUE),2), round(mean(old_abs$word_len, na.rm=TRUE),2))
desc_stats = rbind(conc, word_len)
colnames(desc_stats) = c("min", "max", "mean")
desc_stats
# old conc distribution
hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness")
# new conc distribution
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
# old len distribution
hist(old_abs$word_len, main="Word length of old abstract targets", xlab="Word Length")
# new len distribution
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
(new_abs)
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)&
(matched_abs$word != old_abs$word)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
# filtering out words that are already in old set
matched_abs = subset(matched_abs, matched_abs$word !%in% old_abs$word)
# filtering out words that are already in old set
matched_abs = subset(matched_abs, !(matched_abs$word %in% old_abs$word))
View(matched_abs)
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
# loading in old items
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
# loading in items matched on concreteness and word length from MRC
matched_abs = read.table("conc&len_matched.csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
# filtering out words that are already in old set
matched_abs = subset(matched_abs, !(matched_abs$word %in% old_abs$word))
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
# old conc distribution
hist(old_abs$conc, main="Concreteness values of old abstract targets", xlab="Concreteness")
# new conc distribution
hist(new_abs$conc, main="Concreteness values of new abstract targets", xlab="Concreteness")
# old len distribution
hist(old_abs$word_len, main="Word length of old abstract targets", xlab="Word Length")
# new len distribution
hist(new_abs$word_len, main="Word length of new abstract targets", xlab="Word Length")
(new_abs)
conc = c(round(min(old_abs$conc, na.rm=TRUE), 2), round(max(old_abs$conc, na.rm=TRUE),2), round(mean(old_abs$conc, na.rm=TRUE),2))
word_len = c(round(min(old_abs$word_len, na.rm=TRUE), 2), round(max(old_abs$word_len, na.rm=TRUE),2), round(mean(old_abs$word_len, na.rm=TRUE),2))
desc_stats = rbind(conc, word_len)
colnames(desc_stats) = c("min", "max", "mean")
desc_stats
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
# loading in old items
old_abs = read.table("conc(old_abs).csv", sep=",")
names(old_abs) = c("word", "conc")
old_abs$conc[old_abs$conc=="-"] = NA
old_abs$word_len = NA
i = 1
for (word in old_abs$word){
old_abs$word_len[i] = nchar(word)
i = i + 1
}
# loading in items matched on concreteness and word length from MRC
matched_abs = read.table("match(conc,len).csv", sep=" ")
matched_abs[,4] = NULL
colnames(matched_abs) = c("word", "conc", "word_len")
# filtering out words that are already in old set
matched_abs = subset(matched_abs, !(matched_abs$word %in% old_abs$word))
# sampling items from matched_abs to fit distribution of old items on concreteness and word length
new_abs = data.frame(word=NULL, conc=NULL, word_len=NULL)
row_index = 1
for (row_index in 1:nrow(old_abs)){
row = old_abs[row_index, ]
conc = as.numeric(row$conc)
len = as.numeric(row$word_len)
filtered_abs = matched_abs[((matched_abs$conc<conc+10)&(matched_abs$conc>conc-10)&(matched_abs$word_len<len+1)&(matched_abs$word_len>len-1)),]
sample = sample_n(filtered_abs, 1)
new_abs = rbind(new_abs, sample)
row_index = row_index + 1
}
